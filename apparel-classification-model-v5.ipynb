{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "differential-curtis",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "preliminary-european",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/apparels/Skirt/f4b934fc-e326-4f72-b535-53...</td>\n",
       "      <td>Skirt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/apparels/Skirt/fdbcfdcd-5deb-4a20-9aef-b7...</td>\n",
       "      <td>Skirt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/apparels/Skirt/ecfd00ef-2fe1-4778-a2a8-e9...</td>\n",
       "      <td>Skirt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/apparels/Skirt/2ac96388-4353-44b6-a174-de...</td>\n",
       "      <td>Skirt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/apparels/Skirt/bd289469-607c-45df-bad3-ed...</td>\n",
       "      <td>Skirt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11682</th>\n",
       "      <td>data/apparels/Shirt/7be93c0fe39fd61ea0c5217ea9...</td>\n",
       "      <td>Shirt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11683</th>\n",
       "      <td>data/apparels/Shirt/45279e1f-82af-4fe3-b0ad-2b...</td>\n",
       "      <td>Shirt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11684</th>\n",
       "      <td>data/apparels/Shirt/aaeccde537821c284988d61777...</td>\n",
       "      <td>Shirt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11685</th>\n",
       "      <td>data/apparels/Shirt/ae9cec7a-dd1d-49bc-adae-64...</td>\n",
       "      <td>Shirt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11686</th>\n",
       "      <td>data/apparels/Shirt/0949e8e0-c807-4b6d-8453-80...</td>\n",
       "      <td>Shirt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11687 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   image  label\n",
       "0      data/apparels/Skirt/f4b934fc-e326-4f72-b535-53...  Skirt\n",
       "1      data/apparels/Skirt/fdbcfdcd-5deb-4a20-9aef-b7...  Skirt\n",
       "2      data/apparels/Skirt/ecfd00ef-2fe1-4778-a2a8-e9...  Skirt\n",
       "3      data/apparels/Skirt/2ac96388-4353-44b6-a174-de...  Skirt\n",
       "4      data/apparels/Skirt/bd289469-607c-45df-bad3-ed...  Skirt\n",
       "...                                                  ...    ...\n",
       "11682  data/apparels/Shirt/7be93c0fe39fd61ea0c5217ea9...  Shirt\n",
       "11683  data/apparels/Shirt/45279e1f-82af-4fe3-b0ad-2b...  Shirt\n",
       "11684  data/apparels/Shirt/aaeccde537821c284988d61777...  Shirt\n",
       "11685  data/apparels/Shirt/ae9cec7a-dd1d-49bc-adae-64...  Shirt\n",
       "11686  data/apparels/Shirt/0949e8e0-c807-4b6d-8453-80...  Shirt\n",
       "\n",
       "[11687 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/image_labels.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "quantitative-school",
   "metadata": {},
   "outputs": [],
   "source": [
    "apparel_list = ['T-Shirt','Shirt', 'Dress', 'Pants', 'Shorts','Skirt']\n",
    "apparel_dict = {'T-Shirt':0,'Shirt':1, 'Dress':2, 'Pants':3, 'Shorts':4,'Skirt':5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "lesbian-clarity",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_label = data[\"label\"].to_numpy()\n",
    "boolean_data_label = [label == np.array(apparel_list) for label in data_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "humanitarian-knight",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(data[\"image\"].values)\n",
    "y = boolean_data_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "organized-edwards",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "funded-croatia",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "\n",
    "def process_image(image_path,img_size=IMG_SIZE):\n",
    "    \"\"\"\n",
    "    Take an image file path and turn image into a Tensor.\n",
    "    \"\"\"\n",
    "    image = tf.io.read_file(image_path) # Read image file\n",
    "    image = tf.image.decode_jpeg(image,channels=3) # Turn the image into 3 channels RGB\n",
    "    image = tf.image.convert_image_dtype(image,tf.float32) # Turn the value 0-255 to 0-1\n",
    "    image = tf.image.resize(image,size=[img_size,img_size]) # Resize the image to 224x224\n",
    "    return image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "later-greeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_label(image_path,label):\n",
    "    \"\"\"\n",
    "    Take an image file path name and the associated label,\n",
    "    process the image and return a tuple of (image,label)\n",
    "    \"\"\"\n",
    "    image = process_image(image_path)\n",
    "    return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "union-surfing",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "def create_data_batches(X,y=None,batch_size=BATCH_SIZE,valid_data=False,test_data=False):\n",
    "    \"\"\"\n",
    "    Create batches of data out of image (X) and lebel (y) pairs.\n",
    "    Shuffles the data if it's training data but doesn't shuffle if it's validation data.\n",
    "    Also accepts test data as input (no labels).\n",
    "    \"\"\"\n",
    "    # If the data is a test dataset, we probably don't have labels\n",
    "    if test_data:\n",
    "        print(\"Creating testing data batches...\")\n",
    "        data = tf.data.Dataset.from_tensor_slices((tf.constant(X))) # Only file path (no label)\n",
    "        data_batch = data.map(process_image).batch(batch_size)\n",
    "        return data_batch\n",
    "\n",
    "    # If the data is valid dataset, we don't have to shuffle it\n",
    "    elif valid_data:\n",
    "        print(\"Creating validation data batches...\")\n",
    "        data = tf.data.Dataset.from_tensor_slices((tf.constant(X),tf.constant(y))) # file path, label\n",
    "        data_batch = data.map(get_image_label).batch(batch_size)\n",
    "        return data_batch\n",
    "\n",
    "    else:\n",
    "        print(\"Creating training data batches...\")\n",
    "        data = tf.data.Dataset.from_tensor_slices((tf.constant(X),tf.constant(y))) # file path, label\n",
    "        data = data.shuffle(buffer_size=len(X))\n",
    "        data_batch = data.map(get_image_label).batch(batch_size)\n",
    "        return data_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "multiple-demographic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training data batches...\n",
      "Creating validation data batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-20 13:16:41.063858: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Create training and validation data batches\n",
    "train_data = create_data_batches(X=X_train,y=y_train)\n",
    "val_data = create_data_batches(X=X_test,y=y_test,valid_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cardiovascular-anthropology",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_25_images(images,labels,predicted):\n",
    "    \"\"\"\n",
    "    Display a plot of 25 images and their labels from a batch.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15,10))\n",
    "    for i in range(25):\n",
    "        if apparel_list[labels[i].argmax()] == apparel_list[predicted[i].argmax()]:\n",
    "            color = 'blue'\n",
    "        else:\n",
    "            color = 'red'\n",
    "        ax = plt.subplot(5,5,i+1)\n",
    "        plt.imshow(images[i])\n",
    "        plt.title(apparel_list[labels[i].argmax()],color=color)\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "lovely-playback",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images,train_labels = next(train_data.as_numpy_iterator())\n",
    "# show_25_images(train_images,train_labels,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fresh-while",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images,val_labels = next(val_data.as_numpy_iterator())\n",
    "# show_25_images(val_images,val_labels,val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "interim-freight",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUN_EPOCHS = 100\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\",patience=3)\n",
    "INPUT_SHAPE = [None,IMG_SIZE,IMG_SIZE,3]\n",
    "OUTPUT_SHAPE = 6\n",
    "MODEL_URL = \"https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/4\"\n",
    "\n",
    "def create_model(input_shape=INPUT_SHAPE,output_shape=OUTPUT_SHAPE,model_url=MODEL_URL):\n",
    "    print(f\"Building model with: {model_url}\")\n",
    "    model = tf.keras.Sequential([\n",
    "        hub.KerasLayer(model_url), # Layer 1: Input layer\n",
    "        tf.keras.layers.Dense(units=OUTPUT_SHAPE,activation=\"softmax\")]) # Layer 2: Output layer\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        metrics=[\"accuracy\"])\n",
    "    model.build(input_shape)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "mysterious-thomas",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def create_tensorboard_callback():\n",
    "    logdir = os.path.join(\"logs\",datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    return tf.keras.callbacks.TensorBoard(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "posted-structure",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\",patience=3)\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "def train_model():\n",
    "    \"\"\"\n",
    "    Train a given model and return the trained version.\n",
    "    \"\"\"\n",
    "    model = create_model() # Create a model\n",
    "    model.summary()\n",
    "    tensorboard = create_tensorboard_callback() # Create tensorboard callback\n",
    "    model.fit(x=train_data,\n",
    "              epochs=NUM_EPOCHS,\n",
    "            validation_data=val_data,\n",
    "            validation_freq=1,\n",
    "            callbacks=[tensorboard,early_stopping]) # Fit the model passing it the callback we created\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-assistant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with: https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/4\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer (KerasLayer)    (None, 1001)              5432713   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 6)                 6012      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,438,725\n",
      "Trainable params: 6,012\n",
      "Non-trainable params: 5,432,713\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "293/293 [==============================] - 238s 802ms/step - loss: 0.4163 - accuracy: 0.8569 - val_loss: 0.2703 - val_accuracy: 0.9076\n",
      "Epoch 2/20\n",
      "293/293 [==============================] - 236s 806ms/step - loss: 0.2337 - accuracy: 0.9182 - val_loss: 0.2416 - val_accuracy: 0.9192\n",
      "Epoch 3/20\n",
      "293/293 [==============================] - 329s 1s/step - loss: 0.1994 - accuracy: 0.9317 - val_loss: 0.2421 - val_accuracy: 0.9213\n",
      "Epoch 4/20\n",
      "293/293 [==============================] - 414s 1s/step - loss: 0.1799 - accuracy: 0.9361 - val_loss: 0.2334 - val_accuracy: 0.9192\n",
      "Epoch 5/20\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.1700 - accuracy: 0.9405"
     ]
    }
   ],
   "source": [
    "model = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-journalism",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"models/apparel-classification.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-portland",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted = model.predict(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-prayer",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_predicted = [l.argmax() for l in np.round(test_predicted)]\n",
    "list_y_test = [l.argmax() for l in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-growing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(f\"The accuracy: {accuracy_score(list_predicted,list_y_test)*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-rover",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(list_y_test,list_predicted)\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cm, annot=True, ax = ax,fmt='.0f',cmap='Oranges'); #annot=True to annotate cells\n",
    "\n",
    "ax.set_xlabel('Predicted labels');\n",
    "ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix'); \n",
    "ax.xaxis.set_ticklabels(list(apparel_list)); ax.yaxis.set_ticklabels(list(apparel_list));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-survey",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_25_images(val_images,y_test,test_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-buffer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
